{
  "hash": "5c7bd3ab976b65b317caeb0e9ff5a07a",
  "result": {
    "markdown": "---\ntitle: Simulating GLM\ninstitute: \"University of Padova\"\nauthor: \n  - name: \"Filippo Gambarota\"\n    email: filippo.gambarota@unipd.it\n    github: filippogambarota\nformat:\n  quarto-slides-revealjs:\n    incremental: false\n    code-link: true\n    code-line-numbers: false\n    html-math-method: mathjax\n    code-fold: false\n    filters:\n      - nutshell\n      - code-fullscreen\nfrom: markdown+emoji\ndate: last-modified\ndate-format: \"YYYY\"\nfinal-slide: false\ndf-print: tibble\nbibliography: \"https://raw.githubusercontent.com/filippogambarota/bib-database/main/references.bib\"\ncsl: \"https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl\"\nupdated: \"*Last Update: 2023-11-29*\"\ntoc: true\ntoc-title: Contents\ntoc-depth: 1\nengine: knitr\n---\n\n\n\n\n\n\n\n\n\n\n## Monte Carlo Simulations\n\n> Monte Carlo methods, or Monte Carlo experiments, are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. The underlying concept is to use randomness to solve problems that might be deterministic in principle\n\n## General Workflow\n\nDespite the specific applications, Monte Carlo simulations follows a similar pattern:\n\n1. Define the **data generation process** (DGP)\n2. Use **random numbers sampling** to generate data according to **assumptions**\n3. Calculate a **statistics**, fit a **model** or do some **computations** on the generated data\n4. **Repeat** 2-3 several times (e.g., 10000)\n5. Get a **summary of the results**\n\n## Random numbers in R\n\nIn R there are several functions to generate random numbers and they are linked to specific probability distributions. You can type `?family()` to see available distributions for `glm`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n?family\n```\n:::\n\n\n## Random numbers in R\n\nIn fact, there are other useful distributions not listed in `?family()`, because they are not part of `glm`. For example the `beta` or the `unif` (uniform) distributions. Use `?Distributions` for a complete list:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n?Distributions\n```\n:::\n\n\n## Random numbers in R\n\nHowever, it is always possible to include other distributions with packages. For example the `MASS::mvrnorm()` implement the multivariate normal distribution or the `extraDistr::rhcauchy()` for a series of truncated distributions.\n\n## Random numbers in R\n\nThe general pattern is always the same. There are 4 functions called `r`, `p`, `q` and `d` combined with a distribution e.g. `norm` creating several utilities. For example, `rnorm()` generate number from a normal distribution.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx <- rnorm(1e3)\nhist(x)\n```\n\n::: {.cell-output-display}\n![](binomial-glm-power_files/figure-revealjs/unnamed-chunk-7-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n# Why Monte Carlo Simulations? {.section}\n\n## Why Monte Carlo Simulations?\n\nMonte Carlo simulations are used for several purposes:\n\n- Solve computations impossible or hard to do analytically\n- Estimate the statistical power, type-1 error, type-M error etc.\n\n## Example: standard error\n\nA classical example is estimating the standard error (SE) of a statistics. For example, we know that the SE of a sample mean is:\n\n$$\n\\sigma_\\overline x = \\frac{s_x}{\\sqrt{n_x}}\n$$\n\nWhere $s_x$ is the standard deviation of $x$ and $n_x$ is the sample size.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx <- rnorm(100, mean = 10, sd = 5)\nmean(x) # mean\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 10.97163\n```\n:::\n\n```{.r .cell-code}\nsd(x) / sqrt(length(x)) # se\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 0.5886686\n```\n:::\n\n```{.r .cell-code}\n5 / sqrt(length(x)) # analytically, assuming s = 5\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 0.5\n```\n:::\n:::\n\n\n## Example: standard error\n\nHowever we are not good in deriving the SE analytically. We know that the SE is the standard deviation of the sampling distribution of a statistics.\n\n. . .\n\nThe sampling distribution is the distribution obtained by calculating the statistics (in this case the mean) on all possible (or a very big number) samples of size $n$.\n\n. . .\n\nWe can solve the problems creating a very simple Monte Carlo Simulation\n\n## Example: standard error\n\nWe simulate 10000 samples of size $n$ by a normal distribution with $\\mu = 10$ and $\\sigma = 5$. We calculate the mean $\\overline x$ for each iteration and then we calculate the standard deviation of the vectors of means.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnsim <- 1e4\nmx <- rep(0, 1e4)\n\nfor(i in 1:nsim){\n  x <- rnorm(100, 10, 5)\n  mx[i] <- mean(x)\n}\n```\n:::\n\n\n## Example: standard error\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhist(mx)\n```\n\n::: {.cell-output-display}\n![](binomial-glm-power_files/figure-revealjs/unnamed-chunk-10-1.svg){fig-align='center' width=672}\n:::\n\n```{.r .cell-code}\nsd(mx) # the standard error\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 0.5006916\n```\n:::\n:::\n\n\n# Simulating GLM {.section}\n\n## Workflow\n\nThe general workflow is the following:\n\n1. Define the experimental design:\n    - how many variables?\n    - how many participants/trials?\n    - which type of variables (categorical, numerical)?\n2. Define the probability distribution of the response variable:\n    - Gaussian\n    - Poisson\n    - Binomial\n    - ...\n3. Create the model matrix and define all parameters of the simulation: $\\beta_0$, $\\beta_1$, $\\beta_2$, etc.\n4. Compute the linear predictors $\\eta$ on the link function scale\n5. Apply the inverse of the link function $g^{-1}(\\eta)$ obtaining values on the original scale\n6. Simulate the response variable by sampling from the appropriate distribution\n7. Fit the appropriate model and check the result\n8. In case of estimating statistical properties (e.g., power) repeat the simulation (1-7) several times (e.g., 10000) and summarize the results\n\n## Example with a linear model\n\nLet's simulate a simple linear model (i.e., GLM with a Gaussian random component and identity link function).\n\n$$\n\\hat y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\n$$\n\nIn this example we have:\n\n- 1 predictor $x$ that is numeric\n- 1 response variable $y$ that is numeric\n- 3 parameters: $\\beta_0$, $\\beta_1$ and $\\sigma_{\\epsilon}$\n- Gaussian random component and identity link function\n\n## Example with a linear model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn <- 100\nx <- rnorm(n)\n\ndat <- data.frame(x)\n\nX <- model.matrix(~x, data = dat)\nhead(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>   (Intercept)          x\n#> 1           1 -1.4536667\n#> 2           1 -1.6627647\n#> 3           1  0.9356198\n#> 4           1 -1.1253905\n#> 5           1 -1.4189142\n#> 6           1 -0.7415666\n```\n:::\n:::\n\n\n## Example with a linear model\n\nThen let's define the model parameters and compute the predicted values.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nb0 <- 0\nb1 <- 0.6\nsigma2 <- 1\n\ndat$lp <- b0 + b1*x\n\nplot(dat$x, dat$lp)\n```\n\n::: {.cell-output-display}\n![](binomial-glm-power_files/figure-revealjs/unnamed-chunk-12-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n## Example with a linear model\n\nNow, we are fitting a model with a Gaussian random component and an identity link function. Thus using the $g$ function has no effect.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfam <- gaussian(link = \"identity\")\ndat$lp <- fam$linkinv(dat$lp)\ndat$y <- rnorm(nrow(dat), dat$lp, sqrt(sigma2))\nplot(dat$x, dat$y)\n```\n\n::: {.cell-output-display}\n![](binomial-glm-power_files/figure-revealjs/unnamed-chunk-13-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n## Example with a linear model\n\nNow we can fit the appropriate model using the `glm` function:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit <- glm(y ~ x, family = gaussian(link = \"identity\"), data = dat)\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n#> Call:\n#> glm(formula = y ~ x, family = gaussian(link = \"identity\"), data = dat)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.0800  -0.8376   0.0194   0.7584   3.1701  \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  0.02196    0.10330   0.213    0.832    \n#> x            0.55698    0.10298   5.409 4.49e-07 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for gaussian family taken to be 1.067001)\n#> \n#>     Null deviance: 135.78  on 99  degrees of freedom\n#> Residual deviance: 104.57  on 98  degrees of freedom\n#> AIC: 294.25\n#> \n#> Number of Fisher Scoring iterations: 2\n```\n:::\n:::\n\n\n## Example with a linear model\n\nA faster way, especially with many parameters is using matrix multiplication between the $X$ matrix and the vector of coefficients:\n\n\\begin{equation}\n\\boldsymbol{y} = \n\\begin{bmatrix}\n1 & x_{1} \\\\\n1 & x_{2} \\\\\n1 & x_{3} \\\\\n1 & x_{4} \\\\\n\\vdots & x_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1\n\\end{bmatrix} \n+\n\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\epsilon_3 \\\\\n\\vdots \\\\\n\\epsilon_n\n\\end{bmatrix}\n\\end{equation}\n\n## Example with a linear model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nB <- c(b0, b1)\ny <- X %*% B + rnorm(nrow(dat), 0, sqrt(sigma2))\nplot(dat$x, y)\n```\n\n::: {.cell-output-display}\n![](binomial-glm-power_files/figure-revealjs/unnamed-chunk-15-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n## Example with a linear model\n\nNow let's add another effect, for example a binary variable `group`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngroup <- c(\"a\", \"b\")\nx <- rnorm(n*2)\n\ndat <- data.frame(\n  x = x,\n  group = rep(group, each = n)\n)\n\nX <- model.matrix(~ group + x, data = dat)\nhead(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>   (Intercept) groupb          x\n#> 1           1      0  1.3036322\n#> 2           1      0  2.7934811\n#> 3           1      0 -0.8691226\n#> 4           1      0  1.0234322\n#> 5           1      0 -0.4395868\n#> 6           1      0  0.8852635\n```\n:::\n:::\n\n\n## Example with a linear model\n\nNow the model matrix has another column `groupb` that is the dummy-coded version of the `group` variable. Now let's set the parameters:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nb0 <- 0 # y value when group = \"a\" and x = 0 \nb1 <- 1 # difference between groups\nb2 <- 0.6 # slope of the group\nsigma2 <- 1 # residual variance\n```\n:::\n\n\nThen we can compute the formula adding the new parameters:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat$y <- b0 + b1 * ifelse(dat$group == \"a\", 0, 1) + b2 * dat$x + rnorm(nrow(dat), 0, sqrt(sigma2))\n```\n:::\n\n\n## Example with a linear model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat |> \n  ggplot(aes(x = x, y = y, color = group)) +\n  geom_point() +\n  geom_smooth(method = \"lm\",\n              formula = y ~ x,\n              se = FALSE)\n```\n\n::: {.cell-output-display}\n![](binomial-glm-power_files/figure-revealjs/unnamed-chunk-19-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\n## Example with a linear model\n\nThe same using matrix formulation:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nB <- c(b0, b1, b2)\ndat$y <- X %*% B + rnorm(nrow(dat), 0, sqrt(sigma2))\n```\n:::\n\n\nThen we can fit the model:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit <- lm(y ~ group + x, data = dat)\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n#> Call:\n#> lm(formula = y ~ group + x, data = dat)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -2.75930 -0.77328 -0.02422  0.63481  2.72773 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 0.007024   0.100033   0.070    0.944    \n#> groupb      1.179070   0.141083   8.357 1.14e-14 ***\n#> x           0.617479   0.070763   8.726 1.11e-15 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.993 on 197 degrees of freedom\n#> Multiple R-squared:  0.4035,\tAdjusted R-squared:  0.3975 \n#> F-statistic: 66.63 on 2 and 197 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n# Generalized Linear Models {.section}\n\n## Generalized Linear Models\n\nThe workflow presented before can be applied to GLMs. The only extra steps is performing the **link-function** transformation.\n\nWe simulate data fixing coefficients and computing $\\eta$, then we apply the inverse of the link function (4 and 5 from the workflow slide).\n\n## GLM example\n\nLet's simulate the effect of a continuous predictor on the probability of success, thus using a Binomial model.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nns <- 100 # sample size\nx <- runif(ns) # x predictor\nb0 <- qlogis(0.001) # probability of correct response when x is 0\nb1 <- 10 # increase in the logit of a correct response by unit increase in x\n\ndat <- data.frame(id = 1:ns, x = x)\nhead(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A tibble: 6 × 2\n#>      id     x\n#>   <int> <dbl>\n#> 1     1 0.841\n#> 2     2 0.461\n#> 3     3 0.330\n#> 4     4 0.543\n#> 5     5 0.650\n#> 6     6 0.965\n```\n:::\n:::\n\n\n## GLM example\n\nLet's compute the $\\eta$ by doing the linear combination of predictors and coefficients:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat$lp <- b0 + b1 * dat$x\nggplot(dat, aes(x = x, y = lp)) +\n  geom_line() +\n  ylab(latex(\"\\\\eta\")) +\n  xlab(\"x\")\n```\n\n::: {.cell-output-display}\n![](binomial-glm-power_files/figure-revealjs/unnamed-chunk-23-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n## GLM example\n\nThen we can compute $g^{-1}(\\eta)$ applying the inverse of the link function. Let's use the **logit**:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfam <- binomial(link = \"logit\")\ndat$p <- fam$linkinv(dat$lp)\nggplot(dat, aes(x = x, y = p)) +\n  geom_line() +\n  ylim(c(0, 1)) +\n  ylab(latex(\"p\")) +\n  xlab(\"x\")\n```\n\n::: {.cell-output-display}\n![](binomial-glm-power_files/figure-revealjs/unnamed-chunk-24-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n## GLM example\n\nSo far we have the expected probability of success for each participant and $x$, but we need to include the random component. We can use $p$ or $g^{-1}(\\eta)$ more generally to sample from the $\\mu$ parameter of the probability distribution.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat$y <- rbinom(n = nrow(dat), size = 1, prob = dat$p)\nhead(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A tibble: 6 × 5\n#>      id     x     lp      p     y\n#>   <int> <dbl>  <dbl>  <dbl> <int>\n#> 1     1 0.841  1.51  0.819      0\n#> 2     2 0.461 -2.30  0.0911     0\n#> 3     3 0.330 -3.61  0.0264     0\n#> 4     4 0.543 -1.48  0.186      0\n#> 5     5 0.650 -0.409 0.399      0\n#> 6     6 0.965  2.75  0.940      1\n```\n:::\n:::\n\n\n## GLM example\n\nNow we have simulated a vector of responses with the appropriate random component. We can plot the results.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat |> \n  ggplot(aes(x = x, y = y)) +\n  geom_point(position = position_jitter(height = 0.05)) +\n  stat_smooth(method = \"glm\", \n              method.args = list(family = fam),\n              se = FALSE)\n```\n\n::: {.cell-output-display}\n![](binomial-glm-power_files/figure-revealjs/unnamed-chunk-26-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n## GLM example\n\nFinally we can fit the model and see if the parameters are estimated correctly. Of course, we know the true data generation process thus we are fitting the best model.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit <- glm(y ~ x, data = dat, family = fam)\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n#> Call:\n#> glm(formula = y ~ x, family = fam, data = dat)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.1512  -0.5100  -0.1833   0.4998   2.5647  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)   -5.858      1.112  -5.266 1.40e-07 ***\n#> x              8.453      1.619   5.220 1.79e-07 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 129.489  on 99  degrees of freedom\n#> Residual deviance:  72.287  on 98  degrees of freedom\n#> AIC: 76.287\n#> \n#> Number of Fisher Scoring iterations: 6\n```\n:::\n:::\n\n\n# Power analysis {.section}\n\n## Power analysis\n\nOnce the data generation process and the model has been defined, the power analysis is straightforward.\n\nThe hardest part is fixing plausible values according to your knowledge and/or previous literature.\n\nFor example, there are methods to convert from odds ratio to Cohen's $d$ or other metrics.\n\nThe `effectsize` package is a great resource to understand and compute effect sizes.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nor <- 1.5 # odds ratio\neffectsize::oddsratio_to_d(or)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 0.2235446\n```\n:::\n:::\n\n\n## Power analysis\n\nWe can see the relationship between $d$ and (log) Odds Ratio:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](binomial-glm-power_files/figure-revealjs/unnamed-chunk-29-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n## Power analysis\n\nFor example we can a logistic regression with a binary predictor, fixing the effect size:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn <- 30 # number of subjects\nd <- 0.5 # effect size in cohen's d\nor <- effectsize::d_to_oddsratio(d) # this is beta1\nx <- rep(c(\"a\", \"b\"), each = n)\nxc <- ifelse(x == \"a\", 0, 1)\n\ndat <- data.frame(x = x, xc = xc)\nb0 <- qlogis(0.3) # probability of a\nb1 <- log(or)\n\ndat$lp <- b0 + b1 * dat$xc\ndat$y <- rbinom(nrow(dat), 1, plogis(dat$lp))\n\nhead(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A tibble: 6 × 4\n#>   x        xc     lp     y\n#>   <chr> <dbl>  <dbl> <int>\n#> 1 a         0 -0.847     0\n#> 2 a         0 -0.847     0\n#> 3 a         0 -0.847     0\n#> 4 a         0 -0.847     1\n#> 5 a         0 -0.847     0\n#> 6 a         0 -0.847     0\n```\n:::\n:::\n\n\n## Power analysis\n\nClearly, we need to repeat the sampling process several times, store the results (e.g., the p-value of $\\beta_1$) and then compute the power.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnsim <- 1000\np <- rep(0, nsim)\n\nfor(i in 1:nsim){\n  dat$y <- rbinom(nrow(dat), 1, plogis(dat$lp))\n  fit <- glm(y ~ x, data = dat, family = fam)\n  p[i] <- summary(fit)$coefficients[\"xb\", \"Pr(>|z|)\"]\n}\n\nmean(p <= 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 0.378\n```\n:::\n:::\n\n\n## Power analysis\n\nWith just one condition the power analysis is not really meaningful. We can compute the same for different sample sizes. Here my code is using a series of `for` loops but there could be a nicer implementation.\n\n\n::: {.cell layout-align=\"center\" hash='binomial-glm-power_cache/revealjs/unnamed-chunk-32_c2932ca5577f478875e0ef9c502a293f'}\n\n```{.r .cell-code}\nns <- c(30, 50, 100, 150)\n\npower <- rep(0, length(ns))\n\nfor(i in 1:length(ns)){\n  p <- rep(0, nsim)\n  for(j in 1:nsim){\n    dat <- data.frame(id = 1:ns[i], x = rep(c(\"a\", \"b\"), each = ns[i]))\n    dat$xc <- ifelse(dat$x == \"a\", 0, 1)\n    dat$lp <- b0 + b1 * dat$xc\n    dat$y <- rbinom(nrow(dat), 1, plogis(dat$lp))\n    fit <- glm(y ~ x, data = dat, family = fam)\n    p[j] <- summary(fit)$coefficients[\"xb\", \"Pr(>|z|)\"]\n  }\n  power[[i]] <- mean(p <= 0.05)\n}\n```\n:::\n\n\n## Power analysis\n\nThen we can compute the results:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(ns, power, type = \"b\", ylim = c(0, 1), pch = 19)\n```\n\n::: {.cell-output-display}\n![](binomial-glm-power_files/figure-revealjs/unnamed-chunk-33-1.svg){fig-align='center' width=672}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}