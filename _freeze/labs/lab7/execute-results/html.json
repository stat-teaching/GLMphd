{
  "hash": "1bd61deecfad7ef0d07d7399d4857e4b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab 7\"\nformat: html\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::opts_chunk$set(echo = TRUE,\n                      dev = \"svg\",\n                      fig.width = 7,\n                      fig.asp = 0.618,\n                      fig.align = \"center\",\n                      comment = \"#>\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndevtools::load_all()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> ℹ Loading test\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#> ✔ purrr     1.0.2     \n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(ggeffects)\n```\n:::\n\n\n\n\n\n# Overview\n\nWe want to calculate the power for an interaction effect in a Poisson regression. We have a binary variable, between-subjects (`group`) and a continuous variable `x`. We want to simulate:\n\n- a main effect of `x`\n- a main effect of `group`\n- the interaction `x:group`\n\nThe focus of the power analysis is on the interaction. Suppose that we are not sure if worth using a Poisson model we want to estimate also the type-1 error rate of using a linear model fixing the interaction to 0.\n\n# Data generation process\n\nThe data generation process is simple in this case. The `y` variable is sampled from a poisson distribution:\n\n$$\ny_i \\sim \\text{Poisson}(\\lambda)\n$$\n\nThe linear combination of parameters can be applied on the link function space:\n\n$$\ng(\\lambda) = log(\\lambda) = \\eta =  \\beta_0 + \\beta_1G + \\beta_2X + \\beta_3G \\times X\n$$\nThen applying the inverse of the link function we can see the actual values:\n\n$$\ng^{-1}(\\eta) = \\lambda = e^{\\beta_0 + \\beta_1X + \\beta_2G + \\beta_3X \\times G}\n$$\n\nLet's start by simulating the group main effect. We can simulate that the 2 group has a 50% increase in $y$ compared to group 1. Thus in ratio terms, $\\lambda_2/\\lambda_1 = 1.5$. Then, given that we need to simulate on the log space $\\beta_1 = \\log(1.5)  \\sim 0.41$. Then we need to fix the $\\beta_1$ that is the expected value for the group 1. Let's fix a random value as $\\beta_0 = log(10)$. Thus:\n\n$$\nlog(\\lambda_1) = \\beta_0 = log(10)\n$$\n\n$$\nlog(\\lambda_2) = \\lambda_1 + 1.5 \\lambda_1\n$$\n\n$$\nlog(\\frac{\\lambda_2}{\\lambda_1}) = \\beta_1 = log(1.5)\n$$\n\nIn R:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn <- 1e3\nb0 <- log(10)\nb1 <- log(1.5)\ng <- c(1, 2)\n\nlp <- b0 + b1 * ifelse(g == 1, 0, 1)\ndata.frame(\n  g, lp\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>   g       lp\n#> 1 1 2.302585\n#> 2 2 2.708050\n```\n\n\n:::\n:::\n\n\n\nNow we can simulate some values to see the actual pattern:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngroup <- rep(g, each = n/2)\ny <- rpois(n, lambda = exp(b0 + b1 * ifelse(group == 1, 0, 1)))\nboxplot(y ~ group)\n```\n\n::: {.cell-output-display}\n![](lab7_files/figure-html/unnamed-chunk-5-1.svg){fig-align='center' width=672}\n:::\n\n```{.r .cell-code}\nms <- tapply(y, group, mean)\nms\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>      1      2 \n#>  9.740 15.158\n```\n\n\n:::\n\n```{.r .cell-code}\nms[2]/ms[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>        2 \n#> 1.556263\n```\n\n\n:::\n\n```{.r .cell-code}\nlog(ms[2]/ms[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>         2 \n#> 0.4422873\n```\n\n\n:::\n:::\n\n\n\nWe can fit a simple model now and see if we are able to recovery the parameters:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngroup <- factor(group)\nfit <- glm(y ~ group, family = poisson(link = \"log\"))\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> glm(formula = y ~ group, family = poisson(link = \"log\"))\n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  2.27624    0.01433  158.85   <2e-16 ***\n#> group2       0.44229    0.01837   24.08   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for poisson family taken to be 1)\n#> \n#>     Null deviance: 1616.6  on 999  degrees of freedom\n#> Residual deviance: 1022.3  on 998  degrees of freedom\n#> AIC: 5331.7\n#> \n#> Number of Fisher Scoring iterations: 4\n```\n\n\n:::\n\n```{.r .cell-code}\nexp(coef(fit))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> (Intercept)      group2 \n#>    9.740000    1.556263\n```\n\n\n:::\n\n```{.r .cell-code}\n# group 1\nexp(coef(fit)[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> (Intercept) \n#>        9.74\n```\n\n\n:::\n\n```{.r .cell-code}\n# group 2\nexp(coef(fit)[1] + coef(fit)[2])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> (Intercept) \n#>      15.158\n```\n\n\n:::\n\n```{.r .cell-code}\n# or using predict\npredict(fit, newdata = data.frame(group = c(\"1\", \"2\")), type = \"response\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>      1      2 \n#>  9.740 15.158\n```\n\n\n:::\n:::\n\n\n\nLet's see the effect:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(ggeffect(fit))\n```\n\n::: {.cell-output-display}\n![](lab7_files/figure-html/unnamed-chunk-7-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\nClearly we have a lot of observations, but these can be considered as the true effects.\n\nLet's simulate the `x` effect. We can try different values because it harder to guess a plausible $\\beta$ with a continuous predictor. We simulate `x` as a standardized variable thus $x \\sim \\mathcal{N}(0, 1)$:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx <- rnorm(n)\nhist(x)\n```\n\n::: {.cell-output-display}\n![](lab7_files/figure-html/unnamed-chunk-8-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\nNow $\\beta_0$ is the expected value when $x = 0$ thus the expected value for the mean of $x$. Again, let's fix $\\beta_0 = log(10)$. Then we can try different $\\beta_2$ and see what is the predicted range of $y$:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbetas <- c(1.01, 1.1, 1.5, 2, 5)\nb0 <- log(10)\nlps <- lapply(betas, function(b2) b0 + log(b2) * x)\nys <- lapply(lps, function(l) rpois(n, exp(l)))\nnames(ys) <- paste0(\"b\", betas)\ndd <- data.frame(ys, x)\n\ndd |> \n  pivot_longer(1:length(betas), names_to = \"b\", values_to = \"y\") |> \n  ggplot(aes(x = x, y = y)) +\n  facet_wrap(~b, scales = \"free\") +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](lab7_files/figure-html/unnamed-chunk-9-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\nClearly, the $\\beta_2$ effect depends on the scale of $x$. In this case, we can use $beta_2 = log(1.5)$. Again, let's simulate some data and fit the model:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nb2 <- log(1.5)\nx <- rnorm(n)\ny <- rpois(n, exp(b0 + b2 * x))\nfit <- glm(y ~ x, family = poisson(link = \"log\"))\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> glm(formula = y ~ x, family = poisson(link = \"log\"))\n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  2.30667    0.01034  222.99   <2e-16 ***\n#> x            0.39600    0.00976   40.57   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for poisson family taken to be 1)\n#> \n#>     Null deviance: 2698.2  on 999  degrees of freedom\n#> Residual deviance: 1016.0  on 998  degrees of freedom\n#> AIC: 5125.7\n#> \n#> Number of Fisher Scoring iterations: 4\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(ggeffect(fit))\n```\n\n::: {.cell-output-display}\n![](lab7_files/figure-html/unnamed-chunk-10-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\nNow let's combine the effect of `x` and `group`, without the interaction. In practice we are simulating that the effect of `x` is the same for each group thus the two lines `y ~ x` are parallel (in the link function space).\n\n::: {.callout-important}\n## Important\nFor complex simulations, always create a dataframe with all conditions and subjects and then simulate the effects. I've used vectors without specifing the `data = ` argument and using dataframes just for simplicity.\n:::\n\nNow we need to decide how to code the `group` variable. If we use dummy coding, the $\\beta_0$ will be the expected value for group 1, when $x = 0$. If we use sum to 0 coding e.g. group 1 = -0.5 and group 2 = 0.5, $\\beta_0$ will be the expected value when $x = 0$ averaging over group:\n\n- **dummy-coding**\n    - $\\beta_0$ = expected value of $y$ for group = 1 and x = 0\n    - $\\beta_1$ = effect of $x$ (assumed to be the same between groups)\n    - $\\beta_2$ = effect of group. Given that the two lines are parallel, centering or not $x$ is not affecting the parameter\n- **sum to 0 coding**\n    - $\\beta_0$ = expected value of $y$ when $x = 0$ averaging between groups\n    - $\\beta_1$ = effect of $x$ (assumed to be the same between groups)\n    - $\\beta_2$ = effect of group. Given that the two lines are parallel, centering or not $x$ is not affecting the parameter\n    \nLet's use the sum to 0 coding:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nb0 <- log(5) # y when x = 0 and averaged across groups\nb1 <- log(1.1) # x effect\nb2 <- log(1.2) # group effect\n\nn <- 100 # total\ngroup <- rep(c(\"1\", \"2\"), each = n/2)\nx <- rnorm(n)\n\ndat <- data.frame(\n  id = 1:n,\n  group,\n  x\n)\n\nfilor::trim_df(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>    id group      x\n#> 1   1     1  0.105\n#> 2   2     1  0.507\n#> 3   3     1 -0.662\n#> 4   4     1  0.987\n#> 5 ...   ...    ...\n#> 6  97     2 -0.922\n#> 7  98     2  0.843\n#> 8  99     2  1.098\n#> 9 100     2 -1.328\n```\n\n\n:::\n:::\n\n\n\nLet's set the contrasts:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat$group <- factor(dat$group) # need to be a factor first\ncontrasts(dat$group) <- -contr.sum(2)/2 # otherwise -1 and 1\n\nX <- model.matrix(~x+group, data = dat)\nhead(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>   (Intercept)          x group1\n#> 1           1  0.1045875   -0.5\n#> 2           1  0.5067601   -0.5\n#> 3           1 -0.6620044   -0.5\n#> 4           1  0.9873698   -0.5\n#> 5           1 -0.3926804   -0.5\n#> 6           1  1.8577561   -0.5\n```\n\n\n:::\n\n```{.r .cell-code}\ntail(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>     (Intercept)          x group1\n#> 95            1  0.2794925    0.5\n#> 96            1 -0.3849792    0.5\n#> 97            1 -0.9215839    0.5\n#> 98            1  0.8430339    0.5\n#> 99            1  1.0976529    0.5\n#> 100           1 -1.3280496    0.5\n```\n\n\n:::\n:::\n\n\n\nLet's simulate!\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat$groupc <- contrasts(dat$group)[dat$group] # expand the contrats to have the underlying numeric vector\nhead(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>   id group          x groupc\n#> 1  1     1  0.1045875   -0.5\n#> 2  2     1  0.5067601   -0.5\n#> 3  3     1 -0.6620044   -0.5\n#> 4  4     1  0.9873698   -0.5\n#> 5  5     1 -0.3926804   -0.5\n#> 6  6     1  1.8577561   -0.5\n```\n\n\n:::\n\n```{.r .cell-code}\ndat$lp <- with(dat, b0 + b1 * x + b2 * groupc)\n\ndat |> \n  ggplot(aes(x = x, y = lp, color = group)) +\n  geom_line()\n```\n\n::: {.cell-output-display}\n![](lab7_files/figure-html/unnamed-chunk-13-1.svg){fig-align='center' width=672}\n:::\n\n```{.r .cell-code}\ndat |> \n  ggplot(aes(x = x, y = exp(lp), color = group)) +\n  geom_line()\n```\n\n::: {.cell-output-display}\n![](lab7_files/figure-html/unnamed-chunk-13-2.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\nLet's add the random part:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat$y <- rpois(nrow(dat), exp(dat$lp))\ndat |> \n  ggplot(aes(x = x, y = y, color = group)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](lab7_files/figure-html/unnamed-chunk-14-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\nLet's fit the model:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit <- glm(y ~ group + x, data = dat, family = poisson(link = \"log\"))\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> glm(formula = y ~ group + x, family = poisson(link = \"log\"), \n#>     data = dat)\n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  1.62648    0.04494  36.193  < 2e-16 ***\n#> group1       0.32966    0.08981   3.671 0.000242 ***\n#> x            0.11248    0.04353   2.584 0.009772 ** \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for poisson family taken to be 1)\n#> \n#>     Null deviance: 119.138  on 99  degrees of freedom\n#> Residual deviance:  95.386  on 97  degrees of freedom\n#> AIC: 441.7\n#> \n#> Number of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\n\nLet's simulate the power for these main effects using a vector of $n$. The idea is to generate data, fit the model, extract the p-value and repeat for a lot of times.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(2023)\nns <- c(20, 30, 50, 100, 300, 500, 1000)\nnsim <- 1000 # higher is better, just for example, better 10000\n\npower_group <- rep(NA, length(ns))\npower_x <- rep(NA, length(ns))\n\nfor(i in 1:length(ns)){ # loop over ns\n  p_group <- rep(NA, nsim)\n  p_x <- rep(NA, nsim)\n  for(j in 1:nsim){ # the actual simulation\n    x <- rnorm(ns[i], 0, 1)\n    group <- factor(rep(c(\"1\", \"2\"), each = ns[i]/2))\n    contrasts(group) <- -contr.sum(2)/2\n    dat <- data.frame(x, group)\n    dat$groupc <- contrasts(dat$group)[dat$group]\n    dat$lp <- with(dat, b0 + b1 * x + b2 * groupc)\n    dat$y <- rpois(nrow(dat), exp(dat$lp))\n    fit <- glm(y ~ x + group, data = dat, family = poisson(link = \"log\"))\n    fits <- summary(fit)$coefficients\n    p_group[j] <- fits[\"group1\", 4]\n    p_x[j] <- fits[\"x\", 4]\n  }\n  # calculate power\n  power_x[i] <- mean(p_x <= 0.05)\n  power_group[i] <- mean(p_group <= 0.05)\n}\n\npower <- data.frame(ns, power_x, power_group)\npower <- pivot_longer(power, 2:3, names_to = \"effect\", values_to = \"power\")\n\npower |> \n  ggplot(aes(x = ns, y = power, color = effect)) +\n  geom_line()\n```\n\n::: {.cell-output-display}\n![](lab7_files/figure-html/unnamed-chunk-16-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\nThe approach with the loops is quite clear but i prefer using a more functional way. Let's see a quick example:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# define a data generation function\nsim_data <- function(n, b0 = 0, b1 = 0, b2 = 0){\n  x <- rnorm(n, 0, 1)\n  group <- factor(rep(c(\"1\", \"2\"), each = n/2))\n  contrasts(group) <- -contr.sum(2)/2\n  dat <- data.frame(x, group)\n  dat$groupc <- contrasts(dat$group)[dat$group]\n  dat$lp <- with(dat, b0 + b1 * x + b2 * groupc)\n  dat$y <- rpois(nrow(dat), exp(dat$lp))\n  return(dat)\n}\n\nsim_data(20, b0 = log(5), b1 = log(1), b2 = log(2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>              x group groupc       lp  y\n#> 1  -0.35284869     1   -0.5 1.262864  4\n#> 2  -2.28999996     1   -0.5 1.262864  8\n#> 3   0.24203334     1   -0.5 1.262864  4\n#> 4   1.64342275     1   -0.5 1.262864  6\n#> 5  -1.53507823     1   -0.5 1.262864  2\n#> 6   1.16205905     1   -0.5 1.262864  2\n#> 7   0.39676655     1   -0.5 1.262864  5\n#> 8   1.17331498     1   -0.5 1.262864  4\n#> 9   0.58318712     1   -0.5 1.262864  0\n#> 10  1.06282106     1   -0.5 1.262864  1\n#> 11  0.99715444     2    0.5 1.956012  5\n#> 12 -0.56561737     2    0.5 1.956012 11\n#> 13 -0.41987835     2    0.5 1.956012  7\n#> 14 -0.71605586     2    0.5 1.956012 10\n#> 15  0.55317835     2    0.5 1.956012  7\n#> 16 -0.03143793     2    0.5 1.956012 10\n#> 17 -0.64534515     2    0.5 1.956012  9\n#> 18  0.95722964     2    0.5 1.956012  7\n#> 19 -0.40550891     2    0.5 1.956012  5\n#> 20 -0.90957475     2    0.5 1.956012  6\n```\n\n\n:::\n:::\n\n\n\nThen I would create a function to fit the model that extract also the summary:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit_fun <- function(data){\n  fit <- glm(y ~ x + group, data = data, family = poisson(link = \"log\"))\n  fits <- summary(fit)$coefficients\n  fits <- data.frame(fits)\n  names(fits) <- c(\"b\", \"se\", \"z\", \"p\")\n  fits$param <- rownames(fits)\n  rownames(fits) <- NULL\n  return(fits)\n}\n```\n:::\n\n\n\nFinally a simulation function that iterate a single simulation a certain number of times:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndo_sim <- function(n, b0 = 0, b1 = 0, b2 = 0, nsim = 1){\n  replicate(nsim, {\n    data <- sim_data(n, b0, b1, b2)\n    fit_fun(data)\n  }, simplify = FALSE)\n}\n\ndo_sim(100, nsim = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [[1]]\n#>            b         se         z         p       param\n#> 1 0.03488575 0.09848964 0.3542073 0.7231835 (Intercept)\n#> 2 0.04358547 0.10112888 0.4309894 0.6664761           x\n#> 3 0.16054692 0.19728128 0.8137970 0.4157612      group1\n```\n\n\n:::\n\n```{.r .cell-code}\ndo_sim(100, nsim = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [[1]]\n#>             b        se          z         p       param\n#> 1 -0.11532752 0.1063442 -1.0844739 0.2781548 (Intercept)\n#> 2  0.01408003 0.1025619  0.1372832 0.8908070           x\n#> 3  0.02411496 0.2123412  0.1135670 0.9095810      group1\n#> \n#> [[2]]\n#>             b         se          z         p       param\n#> 1 -0.06857374 0.10435130 -0.6571432 0.5110889 (Intercept)\n#> 2  0.06697914 0.09935546  0.6741364 0.5002246           x\n#> 3  0.29707141 0.20867112  1.4236345 0.1545523      group1\n#> \n#> [[3]]\n#>             b         se           z         p       param\n#> 1  0.03276407 0.09940082  0.32961566 0.7416904 (Intercept)\n#> 2  0.01003833 0.10224772  0.09817652 0.9217921           x\n#> 3 -0.36675430 0.19861945 -1.84651752 0.0648171      group1\n```\n\n\n:::\n:::\n\n\n\nFinally I create a grid of conditions and apply the `do_sim` function to each combination (now you will see the powerful aspect of this method):\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsim <- tidyr::expand_grid(ns, b0, b1, b2, nsim = 1000)\nsim\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 7 × 5\n#>      ns    b0     b1    b2  nsim\n#>   <dbl> <dbl>  <dbl> <dbl> <dbl>\n#> 1    20  1.61 0.0953 0.182  1000\n#> 2    30  1.61 0.0953 0.182  1000\n#> 3    50  1.61 0.0953 0.182  1000\n#> 4   100  1.61 0.0953 0.182  1000\n#> 5   300  1.61 0.0953 0.182  1000\n#> 6   500  1.61 0.0953 0.182  1000\n#> 7  1000  1.61 0.0953 0.182  1000\n```\n\n\n:::\n:::\n\n\n\nEach row is a single simulation condition:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(2023)\nres <- mapply(do_sim, sim$ns, sim$b0, sim$b1, sim$b2, sim$nsim, SIMPLIFY = FALSE)\n```\n:::\n\n\n\nNow we have a list of lists with all model results. I can attach this to the `sim` object to have a nested data structure with all my conditions. You need to be a little it familiar with nested dataframes but the final result is very nice.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsim$data <- res\nsim\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 7 × 6\n#>      ns    b0     b1    b2  nsim data          \n#>   <dbl> <dbl>  <dbl> <dbl> <dbl> <list>        \n#> 1    20  1.61 0.0953 0.182  1000 <list [1,000]>\n#> 2    30  1.61 0.0953 0.182  1000 <list [1,000]>\n#> 3    50  1.61 0.0953 0.182  1000 <list [1,000]>\n#> 4   100  1.61 0.0953 0.182  1000 <list [1,000]>\n#> 5   300  1.61 0.0953 0.182  1000 <list [1,000]>\n#> 6   500  1.61 0.0953 0.182  1000 <list [1,000]>\n#> 7  1000  1.61 0.0953 0.182  1000 <list [1,000]>\n```\n\n\n:::\n:::\n\n\n\nLet's compute the power, visualize effects etc.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsimd <- sim |> \n  unnest(data) |> \n  unnest(data)\n\nfilor::trim_df(simd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>     ns    b0    b1    b2 nsim      b    se       z     p       param\n#> 1   20 1.609 0.095 0.182 1000   1.72 0.097  17.733     0 (Intercept)\n#> 2   20 1.609 0.095 0.182 1000  0.395 0.141   2.806 0.005           x\n#> 3   20 1.609 0.095 0.182 1000 -0.055 0.199  -0.276 0.782      group1\n#> 4   20 1.609 0.095 0.182 1000  1.698 0.112  15.207     0 (Intercept)\n#> 5  ...   ...   ...   ...  ...    ...   ...     ...   ...         ...\n#> 6 1000 1.609 0.095 0.182 1000   0.23 0.028   8.092     0      group1\n#> 7 1000 1.609 0.095 0.182 1000  1.581 0.014 109.759     0 (Intercept)\n#> 8 1000 1.609 0.095 0.182 1000  0.096 0.015   6.599     0           x\n#> 9 1000 1.609 0.095 0.182 1000  0.197 0.029   6.849     0      group1\n```\n\n\n:::\n\n```{.r .cell-code}\nsimd |> \n  ggplot(aes(x = b)) +\n  facet_wrap(~param, scales = \"free\") +\n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](lab7_files/figure-html/unnamed-chunk-23-1.svg){fig-align='center' width=672}\n:::\n\n```{.r .cell-code}\nsimd |> \n  group_by(ns, param) |> \n  summarise(power = mean(p <= 0.05)) |> \n  ggplot(aes(x = ns, y = power, color = param)) +\n  geom_line(lwd = 1)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> `summarise()` has grouped output by 'ns'. You can override using the `.groups`\n#> argument.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](lab7_files/figure-html/unnamed-chunk-23-2.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\nThe second approach is much more flexible and you can easily change the simulation setup just by changing `sim` instead of adding nested loops. Clearly, if you want to stick with the `for` instead of `mapply`:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nres <- vector(mode = \"list\", length = nrow(sim))\nfor(i in 1:length(res)){\n  res[[i]] <- do_sim(sim$ns[i], sim$b0[i], sim$b1[i], sim$b2[i], sim$nsim[i])\n}\n```\n:::\n\n\n\nThe core aspect is creating a function and then iterating across conditions.\n\nLet's complete the simulation setup by including the interaction. Now the contrast coding and the centering is also more relevant. The interaction $\\beta_3$ is the difference in slopes between group 1 and 2. The lines are no longer parallel if $\\beta_3 \\neq 0$. Now, using sum to 0 coding, the parameters are:\n\n- $\\beta_0$ is the expected $y$ when $x = 0$, averaged across groups\n- $\\beta_1$ is difference between groups, when $x = 0$ (lines are no longer parallel)\n- $\\beta_2$ is $x$ slope averaged across groups\n- $\\beta_3$ is difference in slopes between groups\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nb0 <- log(5) # y when x = 0 and averaged across groups\nb1 <- log(1.1) # x effect\nb2 <- log(1.2) # group effect\nb3 <- log(1.1) # difference in slopes\n\ndd <- expand_grid(group = c(-0.5, 0.5), x = rnorm(1000))\n\ndd$lp <- b0 + b1 * dd$group + b2 * dd$x + b3 * dd$group * dd$x\ndd$y <- exp(dd$lp)\n\ndd |> \n  ggplot(aes(x = x, y = lp, color = factor(group))) +\n  geom_line()\n```\n\n::: {.cell-output-display}\n![](lab7_files/figure-html/unnamed-chunk-25-1.svg){fig-align='center' width=672}\n:::\n\n```{.r .cell-code}\ndd |> \n  ggplot(aes(x = x, y = y, color = factor(group))) +\n  geom_line()\n```\n\n::: {.cell-output-display}\n![](lab7_files/figure-html/unnamed-chunk-25-2.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\nNow, let's see the type1 error for a fixed n by using a linear model instead of GLM and fixing $\\beta_3 = 0$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(2023)\nnsim <- 5000\nn <- 500\np_lm <- rep(NA, nsim)\np_glm <- rep(NA, nsim)\n\nfor(i in 1:nsim){\n  x <- rnorm(n)\n  x <- scale(x) # standardizing\n  group <- rep(c(-0.5, 0.5), each = n/2)\n  y <- rpois(n, exp(b0 + b1 * group + b2 * x + 0 * x * group)) # no interaction\n  fit_lm <- lm(y ~ x * group)\n  fit_glm <- glm(y ~ x * group, family = poisson(link = \"log\"))\n  p_lm[i] <- summary(fit_lm)$coefficients[4, 4]\n  p_glm[i] <- summary(fit_glm)$coefficients[4, 4]\n}\n\nmean(p_lm <= 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.0736\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(p_glm <= 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.05\n```\n\n\n:::\n:::\n\n\n\nThe type1 error is higher for the lm, we are finding interactions even if there is fixed to 0 in our simulation.\n\n## Your turn!\n\n1. Do the power analysis for the interaction using a $\\beta_3$ (the one that we defined before or another value) using the `for` or functional approach (you need to define new functions to deal with the interaction).\n2. Create another simulation where you simulate that the one of the two groups has 1/3 of participants of the other group. What happens to the power?\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}