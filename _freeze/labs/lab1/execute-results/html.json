{
  "hash": "f0b25f80f3a02da0531779d780f7c99e",
  "result": {
    "markdown": "---\ntitle: \"Lab 1\"\nauthor: \"Filippo Gambarota\"\nformat: html\nembed-resources: true\n---\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndevtools::load_all()\nlibrary(here)\nlibrary(tidyr) # for data manipulation\nlibrary(dplyr) # for data manipulation\nlibrary(ggplot2) # plotting\nlibrary(performance) # diagnostic\nlibrary(car) # general utilities\nlibrary(MuMIn) # model selection\nlibrary(patchwork)\n```\n:::\n\n\n\n\n# Overview^[The script has been adapted from the Prof. Paolo Girardi (A.Y. 2021/2022) document]\n\nWe are gonna work with the `admission.csv` dataset containing $n = 400$ students for the admission to the UCLA University. A researcher is interested in how variables, such as `gre` (Graduate Record Exam scores), `gpa` (GPA), `rank` (prestige of the undergraduate institution) have an influence for the admission into graduate school. The response variable, admit/don’t admit, is a binary variable.\n\n1. Importing data and check\n2. Exploratory data analysis\n3. Model fitting with `glm()`\n4. Model diagnostic\n5. Interpreting parameters\n6. Model selection\n7. Fitting the model with interactions\n8. Plotting results\n9. Interpreting the parameters\n10. Model comparison for the interactions effect\n\n# 1. Importing data\n\nWe need to set the **working directory** on the root of the course folder using `set.wd()`. Using R Projects is just necessary to open the `.RProj` file and the working directory will be automatically correctly selected.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# reading data\nload(here(\"data\", \"admission.rda\"))\n\n# first rows\nhead(admission)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  admit gre   gpa rank\n1     0 456 5.571    3\n2     1 792 5.637    3\n3     1 960 6.000    1\n4     1 768 5.109    4\n5     0 624 4.823    4\n6     1 912 4.900    2\n```\n:::\n\n```{.r .cell-code}\n# check dataset structure\nstr(admission)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t400 obs. of  4 variables:\n $ admit: int  0 1 1 1 0 1 1 0 1 0 ...\n $ gre  : int  456 792 960 768 624 912 672 480 648 840 ...\n $ gpa  : num  5.57 5.64 6 5.11 4.82 ...\n $ rank : int  3 3 1 4 4 2 1 2 3 2 ...\n```\n:::\n\n```{.r .cell-code}\n# summary statistics\nsummary(admission)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     admit             gre             gpa             rank      \n Min.   :0.0000   Min.   :264.0   Min.   :4.086   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:624.0   1st Qu.:5.043   1st Qu.:2.000  \n Median :0.0000   Median :696.0   Median :5.335   Median :2.000  \n Mean   :0.3175   Mean   :705.5   Mean   :5.329   Mean   :2.485  \n 3rd Qu.:1.0000   3rd Qu.:792.0   3rd Qu.:5.637   3rd Qu.:3.000  \n Max.   :1.0000   Max.   :960.0   Max.   :6.000   Max.   :4.000  \n```\n:::\n:::\n\n\nIt is very important that each variable is correctly interpreted by R:\n\n- `admit` is a binary variable stored as integer (0 and 1)\n- `gre` is a numerical variable stored as integer\n- `gpa` is a numerical variables stored as double precision number\n- `rank` is a numerical variables stored as integer\n\nWe could change the type of `rank` to factor because we are going to use it as a categorical (maybe ordinal) variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadmission$rankc <- factor(admission$rank, levels = 1:4, labels = 1:4)\n```\n:::\n\n\n# 2. Exploratory data analysis\n\nWe can plot the univariate distribution of each variable:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# gre and gpa\nadmission |> \n    select(gre, gpa) |> \n    pivot_longer(1:2) |> \n    ggplot(aes(x = value)) +\n    geom_histogram(col = \"black\",\n                   fill = \"lightblue\") +\n    facet_wrap(~name, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![](lab1_files/figure-html/unnamed-chunk-4-1.svg){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nadmission |> \n    ggplot(aes(x = rank)) +\n    geom_bar()\n```\n\n::: {.cell-output-display}\n![](lab1_files/figure-html/unnamed-chunk-5-1.svg){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nadmission |> \n    ggplot(aes(x = admit)) +\n    geom_bar()\n```\n\n::: {.cell-output-display}\n![](lab1_files/figure-html/unnamed-chunk-6-1.svg){width=672}\n:::\n:::\n\n\nThen we can cut the `gpa` and `gre` variabiles into categories and plot the admissions for each bin (i.e., a contingency table):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadmission$gpa_c <- cut(admission$gpa, seq(4, 6, 0.2), labels = FALSE)\nadmission$gre_c <- cut(admission$gre, seq(260, 960, 50), labels=FALSE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# admission ~ gpa\nadmission |> \n    ggplot(aes(x = gpa_c, fill = factor(admit))) +\n    geom_bar(position = position_dodge()) +\n    labs(fill = \"Admission\") +\n    theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](lab1_files/figure-html/unnamed-chunk-8-1.svg){width=672}\n:::\n\n```{.r .cell-code}\n# admission ~ gre\nadmission |> \n    ggplot(aes(x = gre_c, fill = factor(admit))) +\n    geom_bar(position = position_dodge()) +\n    labs(fill = \"Admission\") +\n    theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](lab1_files/figure-html/unnamed-chunk-8-2.svg){width=672}\n:::\n:::\n\n\nGiven that the number of admitted is lower than the number of non admitted, we can have a look at the proportion of admission for each bin:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadmission |> \n    group_by(gpa_c) |> \n    summarise(admit = mean(admit),\n              non_admit = 1 - admit) |> \n    pivot_longer(2:3) |> \n    ggplot(aes(x = factor(gpa_c), y = value, fill = name)) +\n    geom_col() +\n    labs(fill = \"Admission\",\n         y = \"%\",\n         x = \"gpa\") +\n    theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](lab1_files/figure-html/unnamed-chunk-9-1.svg){width=672}\n:::\n\n```{.r .cell-code}\nadmission |> \n    group_by(gre_c) |> \n    summarise(admit = mean(admit),\n              non_admit = 1 - admit) |> \n    pivot_longer(2:3) |> \n    ggplot(aes(x = factor(gre_c), y = value, fill = name)) +\n    geom_col() +\n    labs(fill = \"Admission\",\n         y = \"%\",\n         x = \"gpa\") +\n    theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](lab1_files/figure-html/unnamed-chunk-9-2.svg){width=672}\n:::\n:::\n\n\nFinally we can have a look at the admissions as a function of the rank of the undergrad institution:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# margin = 2 means that each colum will sum to 1\nprop.table(table(admission$admit, admission$rank), margin = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   \n            1         2         3         4\n  0 0.4590164 0.6423841 0.7685950 0.8208955\n  1 0.5409836 0.3576159 0.2314050 0.1791045\n```\n:::\n:::\n\n\nClearly as the rank of the institute decrease (from 1 to 4) also the proportions of admissions decrease.\n\n# 3. Model fitting with `glm()`\n\nNow we ca fit the model using `glm()`. Let's start by fitting a *null* model with no predictors. We choose a binomial `glm` with a **logit** link function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit0 <- glm(admit ~ 1, data = admission, family = binomial(link = \"logit\"))\nsummary(fit0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = admit ~ 1, family = binomial(link = \"logit\"), data = admission)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-0.8741  -0.8741  -0.8741   1.5148   1.5148  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -0.7653     0.1074  -7.125 1.04e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 499.98  on 399  degrees of freedom\nResidual deviance: 499.98  on 399  degrees of freedom\nAIC: 501.98\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n\nThen we can fit the full model by putting all predictors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit1 <- glm(admit ~ gre + gpa + rankc, family = binomial(link = \"logit\"), data = admission)\nsummary(fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = admit ~ gre + gpa + rankc, family = binomial(link = \"logit\"), \n    data = admission)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.6302  -0.8665  -0.6374   1.1491   2.0833  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -5.1607502  1.5547311  -3.319 0.000902 ***\ngre          0.0019360  0.0009107   2.126 0.033509 *  \ngpa          0.7245343  0.3017552   2.401 0.016347 *  \nrankc2      -0.6755746  0.3165961  -2.134 0.032854 *  \nrankc3      -1.3412412  0.3453868  -3.883 0.000103 ***\nrankc4      -1.5509436  0.4179394  -3.711 0.000207 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 499.98  on 399  degrees of freedom\nResidual deviance: 458.27  on 394  degrees of freedom\nAIC: 470.27\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n\n# 4. Model diagnostic\n\nFirstly we can have a look to the `residual ~ fitted` plot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::residualPlot(fit1)\n```\n\n::: {.cell-output-display}\n![](lab1_files/figure-html/unnamed-chunk-13-1.svg){width=672}\n:::\n:::\n\n\nGiven that the `admit` is a binary variables and we are using a bernoulli model we can use the **binned residuals** to have a better idea:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbinres <- data.frame(performance::binned_residuals(fit1, n_bins = 20))\n\nbinres |> \n    ggplot(aes(x = xbar, y = ybar)) +\n    geom_point() +\n    geom_line(aes(x = xbar, y = 2*se)) +\n    geom_line(aes(x = xbar, y = -2*se)) +\n    ylim(c(-0.5,0.5)) +\n    xlab(\"Binned fitted(fit)\") +\n    ylab(\"Binned residuals(fit)\")\n```\n\n::: {.cell-output-display}\n![](lab1_files/figure-html/unnamed-chunk-14-1.svg){width=672}\n:::\n:::\n\n\nThen we can check each predictors as a function of residuals:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresidualPlots(fit1, tests = FALSE)\n```\n\n::: {.cell-output-display}\n![](lab1_files/figure-html/unnamed-chunk-15-1.svg){width=960}\n:::\n:::\n\n\nThen we can check for influential observations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninfl <- infl_measure(fit1)\nhead(infl)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        dfb.1_     dfb.gre     dfb.gpa      dfb.rnk2     dfb.rnk3      dfb.rnk4\n1  0.001928838 0.054851573 -0.02429314 -0.0005685401 -0.027482122  0.0011303580\n2 -0.032365665 0.030589434  0.01842224  0.0011855080  0.093214421  0.0018889694\n3 -0.043753262 0.047057023  0.03647553 -0.0692282501 -0.067292670 -0.0508865383\n4  0.038875345 0.049085495 -0.05725416 -0.0021779468  0.005243727  0.1674589133\n5 -0.021997746 0.004348061  0.01925034  0.0009501938 -0.001356376 -0.0370388854\n6  0.069347733 0.137651326 -0.12263325  0.0568266645  0.011865765  0.0004088698\n        dffit     cov.r       cook.d        hat\n1 -0.07289093 1.0267984 0.0005671995 0.01604128\n2  0.15369752 0.9935677 0.0044945856 0.01089440\n3  0.11248746 1.0312387 0.0014330276 0.02332145\n4  0.22676575 0.9862142 0.0132208550 0.01672049\n5 -0.05402182 1.0254753 0.0003020368 0.01316096\n6  0.19463628 1.0104116 0.0062497431 0.02134213\n```\n:::\n:::\n\n\nPlotting using `car`\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::influenceIndexPlot(fit1, vars = c(\"Studentized\", \"hat\", \"Cook\"))\n```\n\n::: {.cell-output-display}\n![](lab1_files/figure-html/unnamed-chunk-17-1.svg){width=672}\n:::\n:::\n\n\nPlotting also the dfbeta:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndfbeta_plot(fit1)\n```\n\n::: {.cell-output-display}\n![](lab1_files/figure-html/unnamed-chunk-18-1.svg){width=960}\n:::\n:::\n\n\nCheck if there are observations with high standardized (studentized) residuals:\n\n\n::: {.cell}\n\n```{.r .cell-code}\noutlierTest(fit1) # Testing outliers\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNo Studentized residuals with Bonferroni p < 0.05\nLargest |rstudent|:\n    rstudent unadjusted p-value Bonferroni p\n198 2.110769           0.034792           NA\n```\n:::\n:::\n\n\nFor potentially influential observations we could fir a model subtracting that specific observation and compare coefficients. This is similar to the dfbeta metric that suggest no influential observations on model parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Is 198 really influential?\nfit1_no198 <- update(fit1, subset=-c(198))\ncompareCoefs(fit1, fit1_no198)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCalls:\n1: glm(formula = admit ~ gre + gpa + rankc, family = binomial(link = \n  \"logit\"), data = admission)\n2: glm(formula = admit ~ gre + gpa + rankc, family = binomial(link = \n  \"logit\"), data = admission, subset = -c(198))\n\n             Model 1  Model 2\n(Intercept)    -5.16    -5.25\nSE              1.55     1.56\n                             \ngre         0.001936 0.002097\nSE          0.000911 0.000919\n                             \ngpa            0.725    0.720\nSE             0.302    0.303\n                             \nrankc2        -0.676   -0.675\nSE             0.317    0.317\n                             \nrankc3        -1.341   -1.340\nSE             0.345    0.346\n                             \nrankc4        -1.551   -1.645\nSE             0.418    0.427\n                             \n```\n:::\n:::\n\n\n# 5. Interpreting parameters\n\nFirstly, we can extract model parameters, taking the exponential to interpret them as odds ratios:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbroom::tidy(fit1, exponentiate = TRUE, conf.int = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)  0.00574  1.55         -3.32 0.000902 0.000256     0.115\n2 gre          1.00     0.000911      2.13 0.0335   1.00         1.00 \n3 gpa          2.06     0.302         2.40 0.0163   1.15         3.76 \n4 rankc2       0.509    0.317        -2.13 0.0329   0.272        0.945\n5 rankc3       0.262    0.345        -3.88 0.000103 0.131        0.511\n6 rankc4       0.212    0.418        -3.71 0.000207 0.0907       0.471\n```\n:::\n:::\n\n\nWe can interpret these parameters as: for a unit increase in the `x`, the odds of being accepted in grad school increase by `exp(beta)`. If we multiply the `exp(beta)*100` we obtain the expected increase in percentage. Given that we have multiple parameters, when we intepret a specific parameter we are controlling for other parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbroom::tidy(fit1, exponentiate = TRUE, conf.int = TRUE) |>\n    slice(-1) |> \n    mutate(estperc = estimate * 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 8\n  term   estimate std.error statistic  p.value conf.low conf.high estperc\n  <chr>     <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>   <dbl>\n1 gre       1.00   0.000911      2.13 0.0335     1.00       1.00    100. \n2 gpa       2.06   0.302         2.40 0.0163     1.15       3.76    206. \n3 rankc2    0.509  0.317        -2.13 0.0329     0.272      0.945    50.9\n4 rankc3    0.262  0.345        -3.88 0.000103   0.131      0.511    26.2\n5 rankc4    0.212  0.418        -3.71 0.000207   0.0907     0.471    21.2\n```\n:::\n:::\n\n\nTo better interpret the parameters we need to make sure that the scale is meaningful. For example, the `gre` effect seems to be very small but statistically significant. The reason is that a unit increase in `gre` is very small. We could for example rescale the variable dividing for a constant term:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngre <- admission |> \n  ggplot(aes(x = gre)) +\n  geom_histogram(col = \"black\",\n                 fill = \"dodgerblue\",\n                 bins = 30)\n\ngre100 <- admission |> \n  ggplot(aes(x = gre/100)) +\n  geom_histogram(col = \"black\",\n                 fill = \"dodgerblue\",\n                 bins = 30)\n\ngre | gre100\n```\n\n::: {.cell-output-display}\n![](lab1_files/figure-html/unnamed-chunk-23-1.svg){width=672}\n:::\n:::\n\n\nLet's try fitting the model with the new variable:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadmission$gre100 <- admission$gre/100\nfit2 <- glm(admit ~ gre100 + gpa + rankc, family = binomial(link = \"logit\"), data = admission)\n\nsummary(fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = admit ~ gre100 + gpa + rankc, family = binomial(link = \"logit\"), \n    data = admission)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.6302  -0.8665  -0.6374   1.1491   2.0833  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -5.16075    1.55473  -3.319 0.000902 ***\ngre100       0.19360    0.09107   2.126 0.033509 *  \ngpa          0.72453    0.30176   2.401 0.016347 *  \nrankc2      -0.67557    0.31660  -2.134 0.032854 *  \nrankc3      -1.34124    0.34539  -3.883 0.000103 ***\nrankc4      -1.55094    0.41794  -3.711 0.000207 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 499.98  on 399  degrees of freedom\nResidual deviance: 458.27  on 394  degrees of freedom\nAIC: 470.27\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbroom::tidy(fit2, exponentiate = TRUE, conf.int = TRUE) |>\n    slice(-1) |> \n    mutate(estperc = estimate * 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 8\n  term   estimate std.error statistic  p.value conf.low conf.high estperc\n  <chr>     <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>   <dbl>\n1 gre100    1.21     0.0911      2.13 0.0335     1.02       1.45    121. \n2 gpa       2.06     0.302       2.40 0.0163     1.15       3.76    206. \n3 rankc2    0.509    0.317      -2.13 0.0329     0.272      0.945    50.9\n4 rankc3    0.262    0.345      -3.88 0.000103   0.131      0.511    26.2\n5 rankc4    0.212    0.418      -3.71 0.000207   0.0907     0.471    21.2\n```\n:::\n:::\n\n\nNow the `gre` effect is more meaningful. Notice how the overall model fitting is not changed togheter with other parameters. We are only rescaling variables.\n\nGenerally we can plot the effects for a better overview of the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(effects::allEffects(fit1))\n```\n\n::: {.cell-output-display}\n![](lab1_files/figure-html/unnamed-chunk-26-1.svg){width=672}\n:::\n:::\n\n\nTo interpret the parameters in probability terms we could use the divide by 4 rule that express the maximum slope (i.e., the maximum probability increase):\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(fit2)[c(\"gpa\", \"gre100\")]/4\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       gpa     gre100 \n0.18113356 0.04840012 \n```\n:::\n:::\n\n\nSimilarly we can compute the marginal effects for each variable that represents the average slope:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmarginaleffects::avg_slopes(fit2, \n                            variables = c(\"gpa\", \"gre100\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n   Term Estimate Std. Error    z Pr(>|z|)   S   2.5 % 97.5 %\n gpa      0.1409     0.0573 2.46   0.0139 6.2 0.02867 0.2531\n gre100   0.0376     0.0174 2.17   0.0303 5.0 0.00358 0.0717\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n```\n:::\n:::\n\n\nBeyond the model coefficients, we could use a likelihood ratio test. Let's start by comparing the null model with the current model. We hope that our variables combinations are doing a better job compared to a null model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(fit0, fit1, test = \"LRT\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table\n\nModel 1: admit ~ 1\nModel 2: admit ~ gre + gpa + rankc\n  Resid. Df Resid. Dev Df Deviance  Pr(>Chi)    \n1       399     499.98                          \n2       394     458.27  5   41.702 6.767e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nAs expected from model summary and the deviance reduction, the variables are useful to predict the probability of admission. How useful? we could use some $R^2$-like measures:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance::r2_tjur(fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTjur's R2 \n0.1023616 \n```\n:::\n:::\n\n\nDespite useful, the model has a low $R^2$. Furthermore the correct classification rate is higher than the chance level but relatively low:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - error_rate(fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.71\n```\n:::\n:::\n\n\n# 6. Model selection\n\nWe could try a model comparison starting from the null model and finishing to the overall model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit2 <- update(fit2, na.action = na.fail) # required for mumin\ndredge(fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGlobal model call: glm(formula = admit ~ gre100 + gpa + rankc, family = binomial(link = \"logit\"), \n    data = admission, na.action = na.fail)\n---\nModel selection table \n  (Intrc)    gpa  gr100 rankc df   logLik  AICc delta weight\n8 -5.1610 0.7245 0.1936     +  6 -229.137 470.5  0.00  0.699\n6 -4.9940 0.9564            +  5 -231.438 473.0  2.54  0.196\n7 -1.8330        0.2728     +  5 -232.088 474.3  3.84  0.102\n5  0.1643                   +  4 -237.483 483.1 12.58  0.001\n4 -6.0450 0.6806 0.2279        3 -240.054 486.2 15.68  0.000\n3 -2.9250        0.3016        2 -242.861 489.8 19.26  0.000\n2 -5.8860 0.9556               2 -243.484 491.0 20.51  0.000\n1 -0.7653                      1 -249.988 502.0 31.50  0.000\nModels ranked by AICc(x) \n```\n:::\n:::\n\n\nThe model selection table suggest that the full model is the most appropriate, at least considering the AIC.\n\n# 7. Fitting model with interactions\n\nHere we fit the two 2-way interactions between `gpa`, `gre` and `rankc`. Let's start from only one interaction:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# gpa * rankc + gre\n\nfit1 <- glm(admit ~ gre + gpa + rankc + gpa:rankc, family = binomial(link = \"logit\"), data = admission)\n# this is equivalent to \n# fit1 <- glm(admit ~ gpa * rankc + gre, family = binomial(link = \"logit\"), data = admission)\nsummary(fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = admit ~ gre + gpa + rankc + gpa:rankc, family = binomial(link = \"logit\"), \n    data = admission)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.6997  -0.8669  -0.6426   1.1582   2.1061  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)  \n(Intercept) -6.5706011  3.5059016  -1.874   0.0609 .\ngre          0.0019573  0.0009179   2.132   0.0330 *\ngpa          0.9832606  0.6600694   1.490   0.1363  \nrankc2       0.9869986  4.2059206   0.235   0.8145  \nrankc3       1.2269661  4.5908870   0.267   0.7893  \nrankc4      -1.5086918  5.9313652  -0.254   0.7992  \ngpa:rankc2  -0.3089665  0.7805070  -0.396   0.6922  \ngpa:rankc3  -0.4734705  0.8450248  -0.560   0.5753  \ngpa:rankc4  -0.0057418  1.1023671  -0.005   0.9958  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 499.98  on 399  degrees of freedom\nResidual deviance: 457.86  on 391  degrees of freedom\nAIC: 475.86\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n\n# 8. Plotting results\n\nWith interactions it is even more important and useful to plot the effects before anything else:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(effects::allEffects(fit1))\n```\n\n::: {.cell-output-display}\n![](lab1_files/figure-html/unnamed-chunk-34-1.svg){width=672}\n:::\n:::\n\n\nThe plot on the left represent the main effect of `gre` and the plot on the right is the interaction between `gpa` and `rankc`. In this case we have an interaction between a numerical and a categorical variable. The model is essentially estimating the relationship between `gpa` and `admit` splitting by the level of `rankc`.\n\n# 9. Interpreting the parameters\n\nThe interpretation of interactions (especially with categorical variables) from model parameters is not always easy because it depends on which **contrasts** are used. By default, R uses **dummy coding** where the reference level of the factor (`rankc`) is the first category and all the other categories are compared to the reference. This influence also other parameters:\n\n- `(Intercept)`: log odds of being admitted for `gpa = 0`, `gre = 0` and `rankc` at the reference (i.e., 1)\n- `gpa`: the increase in log odds of being admitted for a unit increase in the `gpa` for people in the reference level of `rankc` (i.e., 1)\n- `gre`: the increase in log odds of being admitted for a unit increase in the `gre` for people in the reference level of `rankc` (i.e., 1)\n- `rankc2`: is the difference in the log odds of being admitted between `rankc2` and `rankc1` (i.e. is the log odds ratio) when `gpa` and `gre` are 0\n- `rankc3`: is the difference in the log odds of being admitted between `rankc3` and `rankc1` (i.e. is the log odds ratio) when `gpa` and `gre` are 0\n- `rankc4`: is the difference in the log odds of being admitted between `rankc4` and `rankc1` (i.e. is the log odds ratio) when `gpa` and `gre` are 0\n- `gpa:rankc2`: the difference between `rankc` 2 and 1 in the rate of increase of the log odds of being admitted for a unit increase in the `gpa`\n- `gpa:rankc3`: the difference between `rankc` 3 and 1 in the rate of increase of the log odds of being admitted for a unit increase in the `gpa`\n- `gpa:rankc4`: the difference between `rankc` 4 and 1 in the rate of increase of the log odds of being admitted for a unit increase in the `gpa`\n\nFor complex interactions like this, a suggestion is to plot the effects (as we did before) and to estimate the individual slopes checking if the interpretation is correct:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# emmeans is an options to estimate effects regardless the model parameters\nemm <- data.frame(emmeans::emtrends(fit1, ~rankc, var = \"gpa\"))\nemm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  rankc gpa.trend        SE  df  asymp.LCL asymp.UCL\n1     1 0.9832606 0.6600694 Inf -0.3104517  2.276973\n2     2 0.6742941 0.4370205 Inf -0.1822504  1.530839\n3     3 0.5097901 0.5621005 Inf -0.5919067  1.611487\n4     4 0.9775188 0.8931212 Inf -0.7729666  2.728004\n```\n:::\n\n```{.r .cell-code}\n# reference level\nemm$gpa.trend[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9832606\n```\n:::\n\n```{.r .cell-code}\n# gpa:rankc2\nemm$gpa.trend[2] - emm$gpa.trend[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -0.3089665\n```\n:::\n\n```{.r .cell-code}\n# ...\n```\n:::\n\n\nThe other difficulty with interactions is interpreting the categorical variable effects. The `rankc2`, `rankc3` ... effects are interpreted as usual BUT in the presence of interactions by definition the difference between i.e. `rankc2` and `rankc1` depends on the level of other variables (in particular `gpa` in this case). Let's explain this visually:\n\nThis is the main effect of the `rankc` without considering the other variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(effects::effect(\"rankc\", fit1))\n```\n\n::: {.cell-output-display}\n![](lab1_files/figure-html/unnamed-chunk-36-1.svg){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](img/main-effect-rankc.png){width=100%}\n:::\n:::\n\n\nHowever, in the presence of interactions, the odds ratio could be influenced by the `gpa` level where it is evaluated:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lab1_files/figure-html/unnamed-chunk-38-1.svg){width=672}\n:::\n:::\n\n\nThe model (without transformations), evaluate the effect of `rankc` when other variables are 0 and this could be meaningful or not.\n\nWithout interaction by definition the point at which I evaluate the `renkc` effect is not relevant.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lab1_files/figure-html/unnamed-chunk-39-1.svg){width=672}\n:::\n:::\n\n\n# 10. Inference and model comparison for the interactions effect\n\nEven if from the plot there is evidence for a little bit of interaction (i.e, the slopes are not the same across `rankc`) we need a statistical test. The first option is to see the Wald test of model coefficients testing if the slopes are different (e.g., `gpa:rankc2`).\n\nTo test the overall interaction we can use the `car::Anova()` function that reports main effects and interactions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::Anova(fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table (Type II tests)\n\nResponse: admit\n          LR Chisq Df Pr(>Chisq)    \ngre         4.6290  1    0.03144 *  \ngpa         5.9022  1    0.01512 *  \nrankc      21.8339  3  7.063e-05 ***\ngpa:rankc   0.4125  3    0.93765    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nWe see that as reported in the Lab 8, there are the main effects of `gre`, `gpa` and `rankc` but there is no evidence for the interaction. The `gpa:rankc` test if there is at least one slope difference that is statistically significant.\n\nTo note, the `car::Anova(fit1)` results for the interaction is just a likelihood ratio test comparing a model with the interaction vs a model without the interaction:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_noint <- glm(admit ~ gre + gpa + rankc, family = binomial(link = \"logit\"), data = admission)\n\nanova(fit_noint, fit1, test = \"LRT\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table\n\nModel 1: admit ~ gre + gpa + rankc\nModel 2: admit ~ gre + gpa + rankc + gpa:rankc\n  Resid. Df Resid. Dev Df Deviance Pr(>Chi)\n1       394     458.27                     \n2       391     457.86  3  0.41252   0.9376\n```\n:::\n:::\n\n\nIn fact the `Chisq` and the `p values` are the same. The model is just testing if including the interaction reduces the residual deviance.\n",
    "supporting": [
      "lab1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}