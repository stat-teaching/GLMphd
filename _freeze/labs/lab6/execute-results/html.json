{
  "hash": "dfe393ff51f69e402d81460682fb2bf9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab 6\"\nformat: html\n---\n\n::: {.cell}\n\n```{.r .cell-code}\ndevtools::load_all()\nlibrary(tidyverse)\nlibrary(ggeffects)\n```\n:::\n\n\n\n# Overview\n\n# EDA\n\n## Loading data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"psych\")\ndat <- psych\n```\n:::\n\n\n\nGiven that we did not introduced random-effects models, we select a single subject to analyze.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncc <- seq(0, 1, 0.1)\n\ndat$contrast_c <- cut(dat$contrast, cc, include.lowest = TRUE)\n\ndat |> \n  filter(id %in% sample(unique(dat$id), 5)) |> \n  group_by(id, cond, contrast_c) |> \n  summarise(y = mean(y)) |> \n  ggplot(aes(x = contrast_c, y = y, color = cond, group = cond)) +\n  geom_point() +\n  geom_line() +\n  theme(axis.text.x = element_text(angle = 90)) +\n  facet_wrap(~id)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'id', 'cond'. You can override using the\n`.groups` argument.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](lab6_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- filter(dat, id == 6)\n```\n:::\n\n\n\nWe have several interesting stuff to estimate. Let's start by fitting a simple model:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit1 <- glm(y ~ contrast, data = dat, family = binomial(link = \"logit\"))\nsummary(fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = y ~ contrast, family = binomial(link = \"logit\"), \n    data = dat)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -2.11482    0.09237  -22.90   <2e-16 ***\ncontrast     6.41786    0.22165   28.95   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 4038.1  on 2999  degrees of freedom\nResidual deviance: 2500.5  on 2998  degrees of freedom\nAIC: 2504.5\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n:::\n\n\n\nThe parameters `(Intercept)` and `contrast` are respectively the probability of saying yes for stimuli with 0 contrast. Seems odd but in Psychophysics this is a very interesting information. We can call it the false alarm rate. We usually expect this rate to be low, ideally 0.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplogis(coef(fit1)[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n   0.107665 \n```\n\n\n:::\n:::\n\n\n\nThe `contrast` is the slope i.e. the increase in the log odds of saying yes for a unit increase in `contrast`. In this case this parameter is hard to intepret, let's change the scale of the contrast:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat$contrast10 <- dat$contrast * 10\nfit1 <- glm(y ~ contrast10, data = dat, family = binomial(link = \"logit\"))\nsummary(fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = y ~ contrast10, family = binomial(link = \"logit\"), \n    data = dat)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -2.11482    0.09237  -22.90   <2e-16 ***\ncontrast10   0.64179    0.02217   28.95   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 4038.1  on 2999  degrees of freedom\nResidual deviance: 2500.5  on 2998  degrees of freedom\nAIC: 2504.5\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n:::\n\n\n\nNow the `contrast10` is the increase in the log odds of saying yes for an increase of 10% contrast. Using the divide-by-4 rule we obtain an maximal increase of 0.1604465 of probability of saying yes.\n\nAnother interesting parameter is the threshold. In psychophysics the threshold is the required $x$ level (in this case `contrast`) to obtain a certain proportions of $y$ response.\n\nFor a logistic distribution [see @Knoblauch2012-to] the 50% threshold can be estimated as $-\\frac{\\beta_0}{\\beta_1}$ thus:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n-(coef(fit1)[1]/coef(fit1)[2])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n   3.295206 \n```\n\n\n:::\n:::\n\n\n\nThen the slope is simply the inverse of the regression slope and represent the increase in performance/visibility for a unit increase in $x$:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1/coef(fit1)[2]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ncontrast10 \n  1.558152 \n```\n\n\n:::\n:::\n\n\n\nIn fact, we can use these parameters to plot a logistic distribution:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncurve(plogis(x, -(coef(fit1)[1]/coef(fit1)[2]), 1/coef(fit1)[2]),\n      0, 10)\n```\n\n::: {.cell-output-display}\n![](lab6_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\nThat is very similar to the effects estimated by our model:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(ggeffect(fit1))\n```\n\n::: {.cell-output-display}\n![](lab6_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n# References\n",
    "supporting": [
      "lab6_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}