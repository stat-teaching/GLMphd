{
  "hash": "fef7082fea1c128761ab240bb3bbc62b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab 4\"\nformat: html\n---\n\n::: {.cell}\n\n```{.r .cell-code}\ndevtools::load_all() # if using the rproject dowloaded from the slides\n# source(\"utils-glm.R\") # if using a standard setup\nlibrary(here)\nlibrary(tidyr) # for data manipulation\nlibrary(dplyr) # for data manipulation\nlibrary(ggplot2) # plotting\nlibrary(car) # general utilities\nlibrary(effects) # for extracting and plotting effects \nlibrary(emmeans) # for marginal means\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"tantrums\")\ndat <- tantrums\n```\n:::\n\n\n\n# Overview\n\nThe dataset `tantrums.csv` is about the number of tantrums of `nrow(child)` toddlers during two days at the nursery. The columns are:\n\n- `id`: identifier for the child\n- `temperament`: the temperament of the child as \"easy\" or \"difficult\"\n- `attachment`: the attachment of the child as \"secure\" or \"insecure\"\n- `parent_se`: an average self-esteem value of the parents (self report)\n- `parent_skills`: a score representing the teacher judgment about parenting skills\n- `tantrums`: the number of tantrums\n\nWe want to predict the number of tantrums as a function of these predictors.\n\n1. Importing data and check\n    - in the presence of `NA`, remove the children\n    - convert to factors the categorical variable with \"difficult\" and \"insecure\" as reference values\n2. Exploratory data analysis\n3. Model fitting with `glm()`\n4. Diagnostic\n5. Interpreting parameters\n6. Model selection\n7. What about interactions?\n\n# 1. Importing data and check\n\nCheck the structure:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t122 obs. of  6 variables:\n $ id           : int  1 2 3 4 5 6 7 8 9 10 ...\n $ temperament  : chr  \"difficult\" \"easy\" \"difficult\" \"difficult\" ...\n $ attachment   : chr  \"insecure\" \"secure\" \"secure\" \"secure\" ...\n $ parent_se    : int  3 4 9 8 4 6 10 6 10 4 ...\n $ parent_skills: int  5 8 5 6 7 2 9 7 1 7 ...\n $ tantrum      : int  1 0 1 0 0 10 0 2 7 0 ...\n```\n\n\n:::\n:::\n\n\n\nCheck for `NA`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsapply(dat, function(x) sum(is.na(x)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           id   temperament    attachment     parent_se parent_skills \n            0             1             1             0             0 \n      tantrum \n            2 \n```\n\n\n:::\n:::\n\n\n\nSo we have some `NA` values. We managed them according to the instructions:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- dat[complete.cases(dat), ]\ndat$id <- 1:nrow(dat) # restore the id\nrownames(dat) <- NULL\n```\n:::\n\n\n\nLet's convert the categorical variables into factor with the appropriate reference level:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat$temperament <- factor(dat$temperament, levels = c(\"difficult\", \"easy\"))\ndat$temperament[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] difficult easy      difficult difficult difficult\nLevels: difficult easy\n```\n\n\n:::\n\n```{.r .cell-code}\ndat$attachment <- factor(dat$attachment, levels = c(\"insecure\", \"secure\"))\ndat$attachment[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] insecure secure   secure   secure   insecure\nLevels: insecure secure\n```\n\n\n:::\n:::\n\n\n\n# 2. Exploratory data analysis\n\nLet's compute some summary statistics and plots.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       id            temperament    attachment   parent_se     \n Min.   :  1.00   difficult:32   insecure:39   Min.   : 1.000  \n 1st Qu.: 30.25   easy     :86   secure  :79   1st Qu.: 5.000  \n Median : 59.50                                Median : 7.000  \n Mean   : 59.50                                Mean   : 6.364  \n 3rd Qu.: 88.75                                3rd Qu.: 8.000  \n Max.   :118.00                                Max.   :10.000  \n parent_skills       tantrum     \n Min.   : 1.000   Min.   : 0.00  \n 1st Qu.: 5.000   1st Qu.: 0.00  \n Median : 6.000   Median : 1.00  \n Mean   : 6.237   Mean   : 1.72  \n 3rd Qu.: 8.000   3rd Qu.: 2.00  \n Max.   :10.000   Max.   :20.00  \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(dat$temperament)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\ndifficult      easy \n       32        86 \n```\n\n\n:::\n\n```{.r .cell-code}\ntable(dat$attachment)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\ninsecure   secure \n      39       79 \n```\n\n\n:::\n\n```{.r .cell-code}\ntable(dat$attachment, dat$temperament)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          \n           difficult easy\n  insecure        12   27\n  secure          20   59\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1,3))\nhist(dat$parent_se)\nhist(dat$parent_skills)\nhist(dat$tantrum)\n```\n\n::: {.cell-output-display}\n![](lab4_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nLet's compute some bivariate relationships:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(dat$parent_se, dat$tantrum, pch = 19)\n```\n\n::: {.cell-output-display}\n![](lab4_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(dat$parent_skills, dat$tantrum, pch = 19)\n```\n\n::: {.cell-output-display}\n![](lab4_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nboxplot(tantrum ~ temperament, data = dat)\n```\n\n::: {.cell-output-display}\n![](lab4_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\nboxplot(tantrum ~ attachment, data = dat)\n```\n\n::: {.cell-output-display}\n![](lab4_files/figure-html/unnamed-chunk-9-2.png){width=672}\n:::\n:::\n\n\n\n# 3. Model fitting with `glm()`\n\nWe can start by fitting our null model with the `poisson()` family:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit0 <- glm(tantrum ~ 1, family = poisson(link = \"log\"), data = dat)\n```\n:::\n\n\n\nWhat is the intercept here?\n\nThen we can fit a model with the attachment effect:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit1 <- glm(tantrum ~ parent_se, family = poisson(link = \"log\"), data = dat)\nsummary(fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = tantrum ~ parent_se, family = poisson(link = \"log\"), \n    data = dat)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)  \n(Intercept)  0.02631    0.22966   0.115   0.9088  \nparent_se    0.07870    0.03240   2.429   0.0151 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 421.11  on 117  degrees of freedom\nResidual deviance: 415.05  on 116  degrees of freedom\nAIC: 590.21\n\nNumber of Fisher Scoring iterations: 6\n```\n\n\n:::\n:::\n\n\n\nWhat about the overdispersion? What could be the reason?\n\nAssuming that the `attachment` is the only variable that we have, we could estimate the degree of overdispersion:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(residuals(fit1, type = \"pearson\")^2)/fit1$df.residual\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.065601\n```\n\n\n:::\n\n```{.r .cell-code}\nperformance::check_overdispersion(fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Overdispersion test\n\n       dispersion ratio =   5.066\n  Pearson's Chi-Squared = 587.610\n                p-value = < 0.001\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nOverdispersion detected.\n```\n\n\n:::\n:::\n\n\n\nLet's have a look also at the residual plot:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresidualPlots(fit1)\n```\n\n::: {.cell-output-display}\n![](lab4_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Test stat Pr(>|Test stat|)  \nparent_se    3.1933          0.07394 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\nThere is clear evidence of overdispersion. But we have several other variables so before using another model let's fit everything:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_s <- glm(tantrum ~ attachment + temperament + parent_se + parent_skills, family = poisson(link = \"log\"), data = dat)\nsummary(fit_s)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = tantrum ~ attachment + temperament + parent_se + \n    parent_skills, family = poisson(link = \"log\"), data = dat)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(>|z|)    \n(Intercept)       3.19125    0.34888   9.147  < 2e-16 ***\nattachmentsecure -0.05147    0.15964  -0.322    0.747    \ntemperamenteasy  -0.82435    0.14127  -5.835 5.36e-09 ***\nparent_se        -0.01881    0.03605  -0.522    0.602    \nparent_skills    -0.38883    0.03454 -11.257  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 421.11  on 117  degrees of freedom\nResidual deviance: 218.91  on 113  degrees of freedom\nAIC: 400.07\n\nNumber of Fisher Scoring iterations: 6\n```\n\n\n:::\n:::\n\n\n\nLet's check again overdispersion and pearson residuals:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresidualPlots(fit_s)\n```\n\n::: {.cell-output-display}\n![](lab4_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              Test stat Pr(>|Test stat|)    \nattachment                                  \ntemperament                                 \nparent_se        9.5838         0.001963 ** \nparent_skills   29.1366        6.745e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\nThe majority of the distribution seems ok, but there are some values with very high residuals and the overdispersion is still present:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(residuals(fit_s, type = \"pearson\")^2)/fit_s$df.residual\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 8.14082\n```\n\n\n:::\n\n```{.r .cell-code}\nperformance::check_overdispersion(fit_s)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Overdispersion test\n\n       dispersion ratio =   8.141\n  Pearson's Chi-Squared = 919.913\n                p-value = < 0.001\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nOverdispersion detected.\n```\n\n\n:::\n:::\n\n\n\n# 4. Diagnostic\n\nAnother reason for overdispersion could be the presence of outliers and influential points. Let's have a look at the Cook distances:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::influenceIndexPlot(fit_s, vars = c(\"cook\", \"hat\", \"Studentized\"))\n```\n\n::: {.cell-output-display}\n![](lab4_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\nThere are two values (117 and 118) with a very high cook distance and very high studentized residual. We can try to fit a model without these values and check what happens to the model:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_no_out <- dat[-c(117, 118), ]\nfit_no_out <- glm(tantrum ~ attachment + temperament + parent_se + parent_skills, family = poisson(link = \"log\"), data = dat_no_out)\nsummary(fit_no_out)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = tantrum ~ attachment + temperament + parent_se + \n    parent_skills, family = poisson(link = \"log\"), data = dat_no_out)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(>|z|)    \n(Intercept)       3.72884    0.39403   9.463  < 2e-16 ***\nattachmentsecure -0.41074    0.16838  -2.439   0.0147 *  \ntemperamenteasy  -1.08473    0.15090  -7.188 6.56e-13 ***\nparent_se         0.01940    0.04055   0.478   0.6324    \nparent_skills    -0.53074    0.04109 -12.915  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 382.413  on 115  degrees of freedom\nResidual deviance:  84.882  on 111  degrees of freedom\nAIC: 257.73\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n:::\n\n\n\nThe model seems to be clearly improved, especially in terms of overdispersion:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(residuals(fit_no_out, type = \"pearson\")^2)/fit_no_out$df.residual\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.81442\n```\n\n\n:::\n\n```{.r .cell-code}\nperformance::check_overdispersion(fit_no_out)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Overdispersion test\n\n       dispersion ratio =  0.814\n  Pearson's Chi-Squared = 90.401\n                p-value =  0.924\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNo overdispersion detected.\n```\n\n\n:::\n:::\n\n\n\nWe can also compare the two models in terms of coefficients:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::compareCoefs(fit_s, fit_no_out)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCalls:\n1: glm(formula = tantrum ~ attachment + temperament + parent_se + \n  parent_skills, family = poisson(link = \"log\"), data = dat)\n2: glm(formula = tantrum ~ attachment + temperament + parent_se + \n  parent_skills, family = poisson(link = \"log\"), data = dat_no_out)\n\n                 Model 1 Model 2\n(Intercept)        3.191   3.729\nSE                 0.349   0.394\n                                \nattachmentsecure -0.0515 -0.4107\nSE                0.1596  0.1684\n                                \ntemperamenteasy   -0.824  -1.085\nSE                 0.141   0.151\n                                \nparent_se        -0.0188  0.0194\nSE                0.0360  0.0405\n                                \nparent_skills    -0.3888 -0.5307\nSE                0.0345  0.0411\n                                \n```\n\n\n:::\n:::\n\n\n\nIn fact, there are some coefficients with different values. We can check also the dfbeta plots:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndfbeta_plot(fit_s)\n```\n\n::: {.cell-output-display}\n![](lab4_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n\nThe previous observations seems to do not affect the estimated parameters but they impact the overall model fit, deviance and residuals.\n\nLet's have a look at residuals now:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::residualPlot(fit_no_out)\n```\n\n::: {.cell-output-display}\n![](lab4_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\nThere is still some strange pattern but the majority of the distribution seems to be between -1 and 1.\n\n# 5. Interpreting parameters\n\nBefore anything else, just plot the effects:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(allEffects(fit_no_out))\n```\n\n::: {.cell-output-display}\n![](lab4_files/figure-html/unnamed-chunk-23-1.png){width=960}\n:::\n:::\n\n\n\n\nNow we can interpret model parameters:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit_no_out)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = tantrum ~ attachment + temperament + parent_se + \n    parent_skills, family = poisson(link = \"log\"), data = dat_no_out)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(>|z|)    \n(Intercept)       3.72884    0.39403   9.463  < 2e-16 ***\nattachmentsecure -0.41074    0.16838  -2.439   0.0147 *  \ntemperamenteasy  -1.08473    0.15090  -7.188 6.56e-13 ***\nparent_se         0.01940    0.04055   0.478   0.6324    \nparent_skills    -0.53074    0.04109 -12.915  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 382.413  on 115  degrees of freedom\nResidual deviance:  84.882  on 111  degrees of freedom\nAIC: 257.73\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n:::\n\n\nThe `(Intercept)` is the expected number of tantrums for \"insecure\", \"difficult\" children where parent_skills are rated as 0 and parent self esteem is 0, thus 41.6307958. Similarly to the binomial lab, we could center the two numerical variables to have a more meaningful interpretation or we can use the `predict` function to obtain the values that we want.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit_no_out, newdata = data.frame(attachment = \"insecure\", \n                                         temperament = \"difficult\",\n                                         parent_se = mean(dat$parent_se), \n                                         parent_skills = mean(dat$parent_skills)),\n        type = \"response\") # same as exp(prediction)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1 \n1.719356 \n```\n\n\n:::\n:::\n\n\n\nThe `attachmentsecure` is the expected difference in log number of tantrums between `secure - insecure` attachment, controlling for other variables:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemmeans(fit_no_out, pairwise~attachment)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$emmeans\n attachment  emmean    SE  df asymp.LCL asymp.UCL\n insecure    0.0302 0.152 Inf    -0.268    0.3289\n secure     -0.3805 0.153 Inf    -0.679   -0.0816\n\nResults are averaged over the levels of: temperament \nResults are given on the log (not the response) scale. \nConfidence level used: 0.95 \n\n$contrasts\n contrast          estimate    SE  df z.ratio p.value\n insecure - secure    0.411 0.168 Inf   2.439  0.0147\n\nResults are averaged over the levels of: temperament \nResults are given on the log (not the response) scale. \n```\n\n\n:::\n:::\n\n\n\nIn terms of the response scale, we can intepret it as the multiplicative increase of the number of tantrums from secure to insecure attachment:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp(coef(fit_no_out)[\"attachmentsecure\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nattachmentsecure \n       0.6631612 \n```\n\n\n:::\n:::\n\n\n\nMoving from insecure from secure attachment, there is a decrease in the expected number of tantrums of 33.6838801 %.\n\nThe `temperamenteasy` can be interpreted in the same way:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemmeans(fit_no_out, pairwise~temperament)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$emmeans\n temperament emmean    SE  df asymp.LCL asymp.UCL\n difficult    0.367 0.143 Inf    0.0868     0.648\n easy        -0.717 0.152 Inf   -1.0161    -0.419\n\nResults are averaged over the levels of: attachment \nResults are given on the log (not the response) scale. \nConfidence level used: 0.95 \n\n$contrasts\n contrast         estimate    SE  df z.ratio p.value\n difficult - easy     1.08 0.151 Inf   7.188  <.0001\n\nResults are averaged over the levels of: attachment \nResults are given on the log (not the response) scale. \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nexp(coef(fit_no_out)[\"temperamenteasy\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntemperamenteasy \n      0.3379941 \n```\n\n\n:::\n:::\n\n\n\nSo there is a reduction of the 66.2005908 % by moving from difficult to easy temperament.\n\n`parent_se` and `parent_skills` are interpreted similarly. The coefficient represent the increase/decrease in the log number of tantrums for a unit increase in the predictors.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp(coef(fit_no_out)[4:5])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    parent_se parent_skills \n    1.0195878     0.5881722 \n```\n\n\n:::\n:::\n\n\n\nSo the number of tantrums seems to be unaffected by the parents self-esteem but as the parent skills increases there is a reduction in the number of tantrums.\n\n# 6. Model selection\n\nLet's compare the model with and without the `parent_se` terms that appear to be not very useful:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_no_parent_se <- update(fit_no_out, . ~ . -parent_se)\nsummary(fit_no_parent_se)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = tantrum ~ attachment + temperament + parent_skills, \n    family = poisson(link = \"log\"), data = dat_no_out)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(>|z|)    \n(Intercept)       3.88096    0.23280  16.671  < 2e-16 ***\nattachmentsecure -0.40422    0.16787  -2.408    0.016 *  \ntemperamenteasy  -1.08057    0.15080  -7.166 7.75e-13 ***\nparent_skills    -0.53723    0.03905 -13.757  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 382.413  on 115  degrees of freedom\nResidual deviance:  85.111  on 112  degrees of freedom\nAIC: 255.96\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n\n```{.r .cell-code}\nanova(fit_no_parent_se, fit_no_out, test = \"LRT\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table\n\nModel 1: tantrum ~ attachment + temperament + parent_skills\nModel 2: tantrum ~ attachment + temperament + parent_se + parent_skills\n  Resid. Df Resid. Dev Df Deviance Pr(>Chi)\n1       112     85.111                     \n2       111     84.882  1  0.22948   0.6319\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndrop1(fit_no_out, test = \"LRT\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\ntantrum ~ attachment + temperament + parent_se + parent_skills\n              Df Deviance    AIC     LRT  Pr(>Chi)    \n<none>             84.882 257.73                      \nattachment     1   90.605 261.45   5.724   0.01674 *  \ntemperament    1  136.028 306.87  51.146 8.573e-13 ***\nparent_se      1   85.111 255.96   0.229   0.63191    \nparent_skills  1  304.711 475.56 219.829 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\nOr using the `MuMIn::dredge()` function:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_no_out <- update(fit_no_out, na.action = na.fail)\nMuMIn::dredge(fit_no_out, rank = \"AIC\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nFixed term is \"(Intercept)\"\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGlobal model call: glm(formula = tantrum ~ attachment + temperament + parent_se + \n    parent_skills, family = poisson(link = \"log\"), data = dat_no_out, \n    na.action = na.fail)\n---\nModel selection table \n      (Int) att     prn_se prn_skl tmp df   logLik   AIC  delta weight\n14  3.88100   +            -0.5372   +  4 -123.978 256.0   0.00  0.608\n16  3.72900   +  0.0194000 -0.5307   +  5 -123.863 257.7   1.77  0.251\n13  3.48300                -0.5133   +  3 -126.768 259.5   3.58  0.102\n15  3.38600      0.0118300 -0.5090   +  4 -126.725 261.5   5.49  0.039\n5   2.89700                -0.5160      2 -150.397 304.8  48.84  0.000\n6   3.08300   +            -0.5235      3 -149.436 304.9  48.92  0.000\n7   2.96600     -0.0082560 -0.5189      3 -150.376 306.8  50.80  0.000\n8   3.08500   + -0.0002435 -0.5236      4 -149.436 306.9  50.92  0.000\n11  0.16220      0.1467000           +  3 -234.197 474.4 218.44  0.000\n12  0.06833   +  0.1469000           +  4 -233.778 475.6 219.60  0.000\n9   1.13900                          +  2 -243.108 490.2 234.26  0.000\n10  1.04200   +                      +  3 -242.642 491.3 235.33  0.000\n3  -0.48580      0.1398000              2 -264.575 533.1 277.19  0.000\n4  -0.52520   +  0.1395000              3 -264.498 535.0 279.04  0.000\n1   0.45590                             1 -272.629 547.3 291.30  0.000\n2   0.39690   +                         2 -272.475 549.0 292.99  0.000\nModels ranked by AIC(x) \n```\n\n\n:::\n:::\n\n\n\n# 7. What about interactions?\n\nWe can also have a look at interactions, try by yourself to explore interactions between numerical (`parent_skills` and `parent_se`) and categorical (`attachment` and `temperament`) variables. I'm only interested in 1 continuous variable interacting with 1 categorical variable.\n\n- fit a separate model for each interaction\n- interpret the model parameters and the analysis of deviance table (`car::` something :)) or using a model comparison (Likelihood Ratio Test) for testing the interaction\n- plot the model effects\n- comment the results\n",
    "supporting": [
      "lab4_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}